---
title: "HW 5 updated" 
author: "Kelly Wang, kzw2102"
date: "11/10/19"
output: github_document
---

### Problem 1
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)

set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```
* fill in missing numeric values with the mean of the non-missing values
* character variables, fill in missing values with "virginica"

```{r function}
replace_missing = function(x) {
  if (is.character(x)) {
    x=replace(x, is.na(x), "virginica")
  } else {
    mean_x=mean(x, na.rm=TRUE)
    x=replace(x, is.na(x), mean_x)
  }
}

output = map_dfr(iris_with_missing, replace_missing)

output

```
The new output dataframe contains `r nrow(output)` rows and `r ncol(output)` columns, where each row contains a unique value. Those originally with numeric values were replaced with the mean of the non-missing values and those originally with missing characters were replaced with "virginica". 
Variables include length of Sepal, width of Sepal, length of Petal, width of Petal, and the species. 

## Problem 2

```{r problem2}
#TA wise words
data_problem2=tibble(
  files=list.files(path="./data/data"),
  path=str_c("./data/data/",files)
) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()

## change file names to include control arm and subject ID
## make sure weekly observations are tidy
data_problem2_tidy= 
  data_problem2 %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    week_1:week_8,
    names_to="Week",
    names_prefix="week_",
    values_to="Observation"
  ) %>% 
  select(files, Week, Observation) %>% 
  mutate(
    Arm = str_replace(files,"_[0-9][0-9].csv",""),
    Arm = recode(Arm, "con"="Control", "exp"="Experiment"),
    ID = str_replace(files, ".csv", " ")
  ) %>% 
  select(ID, Arm, Week, Observation, -files)
  

# spaghetti plot by group
data_problem2_tidy %>% 
  ggplot(aes(x = Week, y = Observation, group = ID, color= Arm)) + 
  geom_line() +
  labs(
    title = "Individual Observation over 8-Week Period"
  )
```
Here we have a tidy datafram containing `r nrow(data_problem2_tidy)` rows and `r ncol(data_problem2_tidy)` columns. Variables include the subject ID, control arm, the week in which they were observed, and their observations. A total of `r nrow(data_problem2)` individuals participated in the study. Here we can see that the control arms tend to have lower observations than the experiment arms during this 8 week study period. 

### Problem 3:
```{r simulation}

set.seed(1)

sim_regression = function(n=30, beta0 = 2, beta1=0) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 0, sd=1),
    y = beta0 + beta1 * x + rnorm(n, 0, sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data) %>% 
    broom::tidy() %>% 
    select(term, estimate, p.value) %>% 
    filter(term=="x")

}

##trying this outside of the dataframe
sim_results =
  rerun(100, sim_regression(n=30, beta0=2, beta1=0)) %>% 
  bind_rows()

##Generating 10000 datasets from the model
sim_results = 
  tibble(beta1 =c(0:6)) %>% 
  mutate(
    output_lists=map(.x= beta1, ~rerun(10000, sim_regression(beta1 =.x))),
    estimate_p.value=map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_p.value)

sim_results
head(sim_results) %>% knitr::kable()
```

Here we can see the first few lines of the total `r nrow(sim_results)` rows in our dataset. We will be using this simulated dataset and plot the relationship between power and effect size and see if there is a difference betweeen average betas among the whole sample vs those with p<0.05. 

### Proportion of times the null was rejected (the power of the test) on the y axis and true value of beta1 on the xaxis. Relationship between effect size and power. Here are the first lines of the table of estimates of beta1 and of the resulting p-value.

```{r proportion_plot}
sim_results %>% 
  group_by(beta1) %>% 
  summarize(
    total=n(),
    count_reject=sum(p.value<0.05),
    proportion = count_reject/n()
  ) %>% 
  ggplot(aes(x=beta1, y=proportion)) +
  geom_point()+
  geom_line()+
  theme_classic()+
  labs(
    title = " Power of test vs true value of beta1",
    x="Effect Size",
    y= "Power"
  )
```
We can see that as effect size gets larger, power increases. There have a positive linear relationship. 

### Plot showing the average estimate of beta1 on y axis and true value of beta1 on the xaxis with another plot showing average estimate of beta1 only in samples in which the null was rejected on the y axis and true value of beta1 on the x axis. 
```{r estimate_plot}
overall_df =
sim_results %>% 
  group_by(beta1) %>% 
  summarize(
    beta1_mean= mean(estimate)
  )
  
reject_df=
  sim_results %>% 
  filter(p.value<0.05) %>% 
  group_by(beta1) %>% 
  summarize(
    beta1_mean_reject= mean(estimate)
  )

combined_df =
  left_join(overall_df, reject_df, by = "beta1")

ggplot(data=combined_df, aes(x=beta1, y=beta1_mean, color=beta1_mean)) +
  geom_point(aes(x=beta1, y=beta1_mean, xlab="beta1", ylab="mean", color="blue"))+
  geom_point(aes(x=beta1, y=beta1_mean_reject, xlab="beta1", ylab="mean", color="red")) + scale_color_hue(labels=c('overall beta1', 'null beta1'))
```
Here we can see that sample average for beta1 across tests in which the null is rejected (here we see it is in the color red ) is much lower than the average for beta1 across all the tests. This would make sense since the more extreme the betas are, the smaller the would be. We see that the graphs converge later because the larger the effect size, the larger the power is.